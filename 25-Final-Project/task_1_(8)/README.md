## Итоговое задание №8 

Репозиторий содержит решение задачи по анализу и обработке данных с использованием PySpark, ClickHouse и Airflow. Задача описана в условии ниже, а также дополнена инструкциями по развертыванию проекта и использованию кода.

---

### Условие задания

Есть CSV файл (~300 МБ, 590708 строк) со следующей структурой:

- **ID объекта**: уникальный идентификатор (например, 590707).
- **Координаты**: широта и долгота (например, 55.060199, 32.695577).
- **Год постройки**: год (например, 1953.0).
- **Площадь**: площадь здания в кв. м (например, 585.60).
- **Количество этажей**: число этажей (например, 18).
- **Область и город**: регион и город (например, Смоленская область, Ярцево).
- **Адрес**: полный адрес (например, "ул. Братьев Шаршановых, д. 61").
- **Описание объекта**: описание объекта (например, "Жилой дом в Ярцево, по адресу ул. Братьев Шаршановых, д. 61, 1953 года постройки, под управлением ТСЖ «Шаршановых».").

#### Основные задачи:
1. Настроить окружение с использованием `docker-compose` и добавить поддержку PySpark в Airflow.
2. Загрузить данные в DataFrame PySpark, убедиться в корректности чтения данных.
3. Провести анализ данных:
   - Преобразовать типы данных.
   - Вычислить средний и медианный год постройки.
   - Определить топ-10 областей и городов с наибольшим количеством объектов.
   - Найти здания с максимальной и минимальной площадью в каждой области.
   - Определить количество зданий по десятилетиям.
4. Настроить схему таблицы в ClickHouse и загрузить обработанные данные из PySpark в ClickHouse через Airflow.
5. Выполнить SQL-запрос для вывода топ-25 домов с площадью > 60 кв.м.

#### Дополнительно:
Графики для пунктов 5, 6 и 7.

---

### Структура репозитория

- **`docker-compose.yml`**: Конфигурация для развертывания окружения с Airflow и ClickHouse.
- **`Dockerfile`**: Dockerfile для создания образа Airflow с поддержкой PySpark.
- **`requirements.txt`**: Список необходимых зависимостей.
- **`dags/`**: Папка с DAG-файлами для Airflow.
  - **`main-dag.py`**: Основной DAG-файл, реализующий логику задачи.
  - **`connections.json`**: Конфигурация для автоматической настройки соединения Airflow с ClickHouse.
- **`notebooks/`**: Jupyter Notebook для локальной проверки и отладки. 
  - **`notebook.ipynb`**: Здесь можно увидеть построенные графики.
- **`README.md`**: Описание проекта и инструкции.

---

### Развертывание проекта

#### Шаг 1: Клонирование репозитория

```bash
git clone <URL репозитория>
cd stepik-de/25-Final-Project/task_1_(8)
```

#### Шаг 2: Построение и запуск контейнеров

```bash
docker build -t airflow-with-java .
docker-compose up --build
```

#### Шаг 3: Настройка Airflow
1. Открыть интерфейс Airflow по адресу: [http://localhost:8080](http://localhost:8080).
2. Активировать DAG под названием `russian_houses_analysis`.

---

### Используемые технологии
- **PySpark**: для обработки и анализа данных.
- **ClickHouse**: для хранения и выполнения аналитических запросов.
- **Apache Airflow**: для управления ETL процессами.
- **Docker**: для контейнеризации окружения.
- **Matplotlib**: для визуализации данных (дополнительно).
